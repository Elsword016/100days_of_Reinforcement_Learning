{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Optimality\n",
    "**Bellman Expectation Equation**:\n",
    "Relates the value of a state-action pair to the value of the next state or state-action pair for a given policy $\\pi$. It tells how good it is to be in a state/take an action assuming following policy $\\pi$.\n",
    "\n",
    "**Optimal Value functions($v_{*}$ and $q^{*}$)**\n",
    "- The optimal state-value function, $v_{*}$(s), is simply the maximum possible value function achievable from state s over all possible policies.\n",
    "- The optimal action-value function, $q^{∗}(s,a)$, is the maximum value achievable starting in state s, taking action a, and then following the optimal policy thereafter.\n",
    "- If you know the optimal value function, the MDP is considered \"solved\".\n",
    "\n",
    "**Optimal Policy ($\\pi_{*}$)**\n",
    "- An optimal policy is a policy that is better than or equal to all other policies for all states. \n",
    "-  if you know the optimal action-value function $q_{∗}(s,a)$, you can find the optimal policy by simply choosing the action that maximizes $q_{∗}(s,a)$ in each state.\n",
    "\n",
    "**Bellman Optimality Equation**\n",
    "- This equation provides the recursive definition for the optimal value function. Unlike the expectation equation, it doesn't average over a policy. Instead, it uses a **max** operator to select the best possible action at each step.\n",
    "\n",
    "- This equation is non-linear and doesn't have a simple closed-form solution like the MRP equation did. It must be solved with iterative methods like Value Iteration or Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Value Iteration \n",
    "Use the Bellman Optimality Equation to find v*. Value Iteration works by repeatedly applying this equation to our value function estimate until it stops changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "states = {0: 'C1', 1: 'C2', 2: 'C3', 3: 'Pass', 4: 'Pub', 5: 'FB', 6: 'Sleep'} \n",
    "#Initialize the value function to zeros\n",
    "v = np.zeros(len(states)) \n",
    "\n",
    "#set a small threshold for convergence \n",
    "threshold = 1e-6  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration loop\n",
    "- Calculate the action-values, q(s,a), for every state s and every possible action a. The formula for q(s,a) is the part inside the max of the Bellman Optimality Equation.\n",
    "- Find the new value for each state, v_new[s], by taking the maximum over the action-values.\n",
    "- Check if the value function has converged. If it has, break the loop. Otherwise, update v and continue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {\n",
    "    0: 'study', 1: 'other' # 'other' represents pub/facebook/quit etc.\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immediate rewards for entering each state in the MRP \n",
    "R_mrp = np.array([-2, -2, -2, 10, 1, -1, 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition matrix for the MRP, P[i, j] is P(S'=j | S=i) \n",
    "P_mrp = np.array([\n",
    "    # C1   C2   C3   Pass  Pub   FB    Sleep\n",
    "    [0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0],  # From C1\n",
    "    [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.2],  # From C2\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.4, 0.0, 0.0],  # From C3\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # From Pass (Terminal -> Sleep)\n",
    "    [0.2, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0],  # From Pub\n",
    "    [0.1, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0],  # From FB\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   # From Sleep (Terminal)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards for taking the 'study' action in each state\n",
    "R_study = np.array([-2, -2, -2, 10, 0, 0, 0]) \n",
    "\n",
    "# Transition matrix for the 'study' action, P(s'|s, a='study')\n",
    "P_study = np.array([\n",
    "    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # C1 -> C2\n",
    "    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # C2 -> C3\n",
    "    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],  # C3 -> Pass\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # Pass -> Sleep\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],  # Pub (no study) -> Pub\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],  # FB (no study) -> FB\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   # Sleep -> Sleep\n",
    "])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards for taking the 'other' action (play, quit, etc.)\n",
    "R_other = np.array([-1, -2, 1, 0, 1, -1, 0]) # C2->Sleep reward is -2, not 0\n",
    "\n",
    "# Transition matrix for the 'other' action, P(s'|s, a='other')\n",
    "P_other = np.array([\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],  # C1 -> FB\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # C2 -> Sleep\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],  # C3 -> Pub\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # Pass -> Sleep\n",
    "    [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0],  # Pub -> C1 or Pub\n",
    "    [0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0],  # FB -> C1 or FB\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   # Sleep -> Sleep\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function converged!\n",
      "\n",
      "Optimal State-Value Function (v*):\n",
      "  v*(C1) = 1.9\n",
      "  v*(C2) = 4.3\n",
      "  v*(C3) = 7.0\n",
      "  v*(Pass) = 10.0\n",
      "  v*(Pub) = 3.3\n",
      "  v*(FB) = 0.0\n",
      "  v*(Sleep) = 0.0\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    delta = 0  # To track the change in the value function in this iteration\n",
    "\n",
    "    # Store the old value function to check for convergence\n",
    "    v_old = v.copy()\n",
    "\n",
    "    # --- Main Bellman Update Loop ---\n",
    "    for s in range(len(states)):\n",
    "        # Calculate the value of taking each action from state s\n",
    "        # We have two main actions: 'study' and 'other' (play)\n",
    "\n",
    "        # q(s, 'study')\n",
    "        q_study = R_study[s] + gamma * (P_study[s] @ v_old)\n",
    "\n",
    "        # q(s, 'other')\n",
    "        q_other = R_other[s] + gamma * (P_other[s] @ v_old)\n",
    "\n",
    "        # The new value for state s is the max over the action-values\n",
    "        v[s] = max(q_study, q_other)\n",
    "\n",
    "    # Check for convergence\n",
    "    delta = np.max(np.abs(v - v_old))\n",
    "    if delta < threshold:\n",
    "        print(\"Value function converged!\")\n",
    "        break\n",
    "\n",
    "print(\"\\nOptimal State-Value Function (v*):\")\n",
    "for i in range(len(states)):\n",
    "    print(f\"  v*({states[i]}) = {v[i]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (pi*):\n",
      "  In state C1, take action: study\n",
      "  In state C2, take action: study\n",
      "  In state C3, take action: study\n",
      "  In state Pass, take action: study\n",
      "  In state Pub, take action: other\n",
      "  In state FB, take action: study\n",
      "  In state Sleep, take action: other\n"
     ]
    }
   ],
   "source": [
    "# Create an empty policy array\n",
    "optimal_policy = np.zeros(len(states), dtype=int) \n",
    "for s in range(len(states)):\n",
    "    # Recalculate the q-values using the final optimal v*\n",
    "    q_study = R_study[s] + gamma * (P_study[s] @ v)\n",
    "    q_other = R_other[s] + gamma * (P_other[s] @ v)\n",
    "\n",
    "    # The optimal action is the one with the higher q-value\n",
    "    if q_study > q_other:\n",
    "        optimal_policy[s] = 0  # 0 corresponds to 'study'\n",
    "    else:\n",
    "        optimal_policy[s] = 1  # 1 corresponds to 'other'\n",
    "\n",
    "print(\"\\nOptimal Policy (pi*):\")\n",
    "for i in range(len(states)):\n",
    "    action = 'study' if optimal_policy[i] == 0 else 'other'\n",
    "    print(f\"  In state {states[i]}, take action: {action}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
