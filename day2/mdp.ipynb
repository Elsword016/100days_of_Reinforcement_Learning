{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process\n",
    "A MDP is a Markov Reward process with decisions.\n",
    "- Its a tuple (S,A,P,R $\\gamma$)\n",
    "- A is finite set of actions.\n",
    "- P is state transition probability.\n",
    "- R is a reward function for a given action\n",
    "- $\\gamma$ is a discount factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actions \n",
    "actions = {0: 'study', 1: 'pub', 2: 'facebook', 3: 'quit', 4: 'sleep'} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy \n",
    "A policy defines the behavior of an agent within an MDP.\n",
    "Definition: A policy π is a distribution over actions given states, expressed as π(a|s) = P[At = a | St = s]. This means for any given state s, the policy specifies the probability of taking each possible action a.\n",
    "Characteristics of MDP Policies: They depend only on the current state, not the entire history. This is consistent with the Markov property of the environment.\n",
    "They are stationary (time-independent), meaning At ~ π(·|St) for all time steps t > 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the 50/50 random policy from the slides where $\\pi(a|s) = 0.5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the rewards and transitions for the two main actions: \"study\" and \"pub/facebook\"\n",
    "We will assume a simplified MDP based on the diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R_study reward for studying in state s \n",
    "R_study = np.array([-2, -2, -2, 10, 0, 0, 0]) # R=-2 for classes, R=+10 for pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P_study[s, s'] is the transition prob from s to s' if you \"study\"\n",
    "P_study = np.array([\n",
    "    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # C1 -> C2\n",
    "    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # C2 -> C3\n",
    "    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],  # C3 -> Pass\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # Pass -> Sleep\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],  # Pub -> Pub (no study)\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],  # FB -> FB (no study)\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   # Sleep -> Sleep\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_other[s] is reward for playing/quitting\n",
    "R_other = np.array([-1, -1, 1, 0, 1, -1, 0]) # R=-1 for FB, R=+1 for Pub \n",
    "\n",
    "# P_other[s, s'] is transition if you \"play\" (go to pub/facebook)\n",
    "P_other = np.array([\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],  # C1 -> FB\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # C2 -> Sleep\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],  # C3 -> Pub\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # Pass -> Sleep\n",
    "    [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0],  # Pub -> C1 or Pub (simplified)\n",
    "    [0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0],  # FB -> C1 or FB (simplified)\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   # Sleep -> Sleep\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every state, there's a 0.5 probability of \"studying\" and 0.5 of \"playing\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_prob = 0.5  #policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P_pi[s, s'] = pi(study|s)*P(s'|s, study) + pi(play|s)*P(s'|s, play)\n",
    "P_pi = pi_prob * P_study + (1 - pi_prob) * P_other #policy-averaged transitions\n",
    "\n",
    "# R_pi[s] = pi(study|s)*R(s, study) + pi(play|s)*R(s, play)\n",
    "R_pi = pi_prob * R_study + (1 - pi_prob) * R_other #policy-averaged rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the resulting MRP defined by $(S,P_{\\pi},R_{\\pi},\\gamma)$ using the Bellman's Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculated State-Value Function for the 50/50 Policy (v_pi):\n",
      "  v_pi(C1) = -3.8\n",
      "  v_pi(C2) = -0.9\n",
      "  v_pi(C3) = 1.3\n",
      "  v_pi(Pass) = 5.0\n",
      "  v_pi(Pub) = -1.1\n",
      "  v_pi(FB) = -4.2\n",
      "  v_pi(Sleep) = 0.0\n"
     ]
    }
   ],
   "source": [
    "states = {0: 'C1', 1: 'C2', 2: 'C3', 3: 'Pass', 4: 'Pub', 5: 'FB', 6: 'Sleep'} \n",
    "gamma = 0.9  \n",
    "v_pi = np.linalg.inv(np.eye(len(states)) - gamma * P_pi) @ R_pi \n",
    "\n",
    "print(\"\\nCalculated State-Value Function for the 50/50 Policy (v_pi):\")\n",
    "for i in range(len(states)):\n",
    "    print(f\"  v_pi({states[i]}) = {v_pi[i]:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
