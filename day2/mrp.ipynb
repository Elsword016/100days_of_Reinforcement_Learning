{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Reward Process (Lecture-2 till Bellman's Expectation Eq)\n",
    "Like previous one, I will code the Student MDP from the lecture slides which will help to understand abstract concepts of an MRP-states,transitions and rewards and also use Bellman Equation.\n",
    "\n",
    "#### Markov Reward process\n",
    "A Markov reward process is a tuple $(S,P,R,\\gamma)$:\n",
    "- S is a finite set of states.\n",
    "- P is a state transition probability matrix \n",
    "- R is the reward function $R_{s} = E[R_{t+1} | S_{t} = s]$\n",
    "- $\\gamma$ is a discount factor, $\\gamma [0,1]$\n",
    "- $\\gamma$ close to 0 leads \"myopic\" evaluation\n",
    "- $\\gamma$ close to 1 leads \"far-sighted\" evaluation\n",
    "\n",
    "#### Return \n",
    "The return $G_{t}$ is the total discounted reward from time-step t.\n",
    "$G_{t} = R_{t+1}+\\gamma R{t+2}+.... = \\Sigma {\\gamma}^{k} R_{t+k+1} k[0,\\inf]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "states = {0: 'C1', 1: 'C2', 2: 'C3', 3: 'Pass', 4: 'Pub', 5: 'FB', 6: 'Sleep'} \n",
    "#immediate rewards R(t+1) for entering each state \n",
    "R = np.array([-2, -2, -2, 10, 1, -1, 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the transition matrix P[i,j] from moving state i to state j \n",
    "P = np.array([\n",
    "    # C1   C2   C3   Pass  Pub   FB    Sleep\n",
    "    [0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0],  # Transitions from C1\n",
    "    [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.2],  # Transitions from C2\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.4, 0.0, 0.0],  # Transitions from C3\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # Transitions from Pass\n",
    "    [0.2, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0],  # Transitions from Pub\n",
    "    [0.1, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0],  # Transitions from FB\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]   # Transitions from Sleep\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discount factor \n",
    "gamma = 0.9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the Bellman's Equation\n",
    "Bellman's Equation is linear so can be solved easily. And also we will use the matrix representation to do so:\n",
    "\n",
    "$v = (I-\\gamma P)^{-1} R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(len(states)) #len same size as P \n",
    "v = np.linalg.inv(I - gamma * P) @ R  #bellman's equation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated State-Value Function (v):\n",
      "v(C1): -5.0\n",
      "v(C2): 0.9\n",
      "v(C3): 4.1\n",
      "v(Pass): 10.0\n",
      "v(Pub): 1.9\n",
      "v(FB): -7.6\n",
      "v(Sleep): 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculated State-Value Function (v):\")\n",
    "for i in range(len(states)): \n",
    "    print(f\"v({states[i]}): {v[i]:.1f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
